<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <!-- meta tags--><meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--external style sheets--><link href="../style.css" type="text/css" rel="stylesheet" />
<title>DCCPS: Small Grants Program: About</title>
</head>
<body>
<div id="skipmenu">
<a href="#skip" class="skippy">Skip Navigation</a>
<a name="top"></a>
</div> <!-- end skipmenu -->
<div id="wrapper">
<!-- NCI Banner -->
  <div id="head-row1"><img src="../images/dccps-banner2.gif" alt="National Cancer Institute" width="1000" height="40" usemap="#Map" /></div>
<!-- end of NCI Banner -->

<!-- DCCPS Banner / Search Field -->
<div id="head-row2">
  <div id="left2"><a href="../index.html"><img src="../images/dccps-banner.gif" alt="Cancer Control &amp; Population Sciences Home - NCI's Bridge to Public Health Research, Practice and Policy" width="730" height="68" /></a></div>
  
  <div id="right2">
	<div id="north">
	<a href="http://twitter.com/NCICancerCtrl" title="Twitter">Twitter<img src="../images/twitter-icon.png" title="Twitter" alt="Twitter" width="20" height="20" /></a><a href="../exit_disclaimer.html"><img src="../images/Icon_External_Link.png" width="12" height="12" alt="exit disclaimer" /></a> <a href="../cr-communication-videos.html" title="Multimedia">Multimedia<img src="../images/media-icon.gif" title="Multimedia" alt="Multimedia" width="20" height="20" /></a></div>
    <form method="get" action="http://search2.google.cit.nih.gov/search" name="search">
	  <input type="hidden" name="site" value="DCCPS" />
	  <input type="hidden" name="client" value="DCCPS_frontend" />
	  <input type="hidden" name="proxystylesheet" value="DCCPS_frontend" />
  	  <input type="hidden" name="output" value="xml_no_dtd" />
	  <input type="hidden" name="filter" value="0" />
	  <input type="hidden" name="getfields" value="*" />
	  <label for="searchbox"><input id="searchbox" type="text" name="q" size="15" maxlength="255" class="htextf" value="Search" /></label>
	  <input type="image" src="../images/hbutton.gif"  class="hbutton" name="btnG" id="btnG" alt="Search" />
	</form><br class="clearfloat" />
    </div><br class="clearfloat" />
  </div>
<!-- End of DCCPS Banner / Search Field -->
	
<!-- Main Content -->
  <div id="mainarea">
  <!-- Left Column // DCCPS Link / Need Help Banner -->
  <div id="column-left1">
    <div id="dccps-link"><a href="../index.html">Cancer Control &amp; Population Sciences Home</a></div>
    <div class="rounded-border">
      <ul class="slist2">
        <li><a href="../funding.html" style="font-weight:bold;">Funding Opportunities Home</a></li>
      </ul>
    </div>
    <a href="http://www.cancer.gov/help"><img src="../images/help-tile.jpg" alt="Need Help? Contact us by phone (1-800-422-6237), Web, or e-mail" /></a> </div>
  <!-- End of Left Column // DCCPS Link / Need Help Banner -->
    
  <!-- Right Column // Page Content -->
    <div id="column-mid2">
	<a name="skip"></a>
      <div class="content">
        <h2>Step-By-Step Grant Help</h2>
        <h3>The Grant Writing Process - Design</h3>
        <h3>Analytic Techniques</h3>
        <p><strong>Topics</strong></p>
        <ol>
          <li>Poor Analyses = Virtually Worthless Findings</li>
          <br />
          <br />
          <li>The meaning of p (one versus <a href="desgloss.html#Two-Tailed">two-tailed</a> <a href="desgloss.html#Hypothesis">hypothesis</a>)</li>
          <br />
          <br />
          <li>General Considerations</li>
          <ul>
            <li><a href="desgloss.html#Experimenter-Expectancy-Effects">multiple comparisons</a> may yield <a href="desgloss.html#Experimenter-Expectancy-Effects">spurious results</a></li>
            <li><a href="desgloss.html#Adjustments-For-Multiple-Comparisons">adjustments for multiple comparisons</a> (e.g., Bonferroni)</li>
            <li>limitations of adjustments </li>
            <li><a href="desgloss.html#Clinical-Significance">clinical significance</a> versus <a href="desgloss.html#Statistical Significance">statistical significance</a> may be of some consideration when interpreting the results</li>
          </ul>
          <br />
          <li>Consider the Distribution of the Data when Choosing the Data Analytic Technique</li>
          <ul>
            <li><a href="desgloss.html#Parametric">parametric</a></li>
            <li>the data can be influenced by <a href="desgloss.html#Outliers">outliers</a></li>
            <li>methods to remove <a href="desgloss.html#Skewness">skewness</a> from the distribution include </li>
            <li><a href="desgloss.html#Transformations">transformations</a></li>
            <li>log linear</li>
            <li>square root</li>
            <li>exponential</li>
            <li>rank</li>
            <li>others</li>
            <li>trimming / problems with trimming </li>
            <li><a href="desgloss.html#Nonparametric">nonparametric</a> (&quot;distribution free methods&quot;)</li>
          </ul>
          <br />
          <li>Temporal Nature of the Data (Retrospective versus Longitudinal)Will Also Influence the Selection of the Appropriate Analysis</li>
          <ul>
            <li>for <a href="desgloss.html#Nominal-Data">independent samples</a></li>
            <li>for <a href="desgloss.html#Paired-Samples">paired samples</a></li>
          </ul>
          <br />
          <li>Cite Both the <a href="desgloss.html#Test Statistic">Test Statistic</a> and the p-Value when Reporting Results</li>
          <br />
          <br />
          <li>Before Submitting the Grant Application, Estimate Power of the Test   to Support the Number of Subjects Needed for the Analysis. Power   Depends on the:</li>
          <ul>
            <li>expected magnitude of the <a href="desgloss.html#Relationship">relationship</a> <a href="desgloss.html#Effect-Size">(effect size</a>)</li>
            <li>number of subjects <a href="desgloss.html#Ordinal Data">(sample size</a>)</li>
            <li>variability of the data</li>
            <li>level of <a href="desgloss.html#Alpha">alpha</a></li>
            <li>type of <a href="desgloss.html#Hypothesis">hypothesis</a> <a href="desgloss.html#One-Tailed">(one-tailed</a> versus <a href="desgloss.html#Two-Tailed">two-tailed</a>)</li>
          </ul>
          <br />
          <li>Using <a href="desgloss.html#Multivariate-Analyses">Multivariate Analyses</a> Will Usually Increase the Appeal of the Study</li>
          <ul>
            <li>be specific about the <a href="desgloss.html#Dependent-Variables">independent variables</a> and <a href="desgloss.html#Dependent-Variables">dependent variables</a></li>
            <li>judiciously select the variables to be included in the analysis</li>
            <li>be specific about the analyses that is being used</li>
            <li>the benefits of multivariate analysis are these analyses:</li>
            <li>enables the building of <a href="desgloss.html#Models">models</a></li>
            <li>can be used to <a href="desgloss.html#Control">control</a> for <a href="desgloss.html#Confounding-Variables">confounding variables</a>: let variables &quot;fight it out,&quot; or develop a more parsimonious model</li>
            <li>considerations</li>
            <li>avoid <a href="desgloss.html#Colinearity">colinearity</a> (or <a href="desgloss.html#Multicollinearity">multicollinearity</a>)</li>
            <li>maintain a high <a href="desgloss.html#Case-Variable-Ratio">case/variable ratio</a></li>
            <li>number of variables in the equation</li>
            <li>comparison of within-group versus <a href="desgloss.html#Between-Group">between-group</a> analyses</li>
          </ul>
        </ol>
        <p><strong>Discussion</strong></p>
        <p><em>Data Analysis</em></p>
        <p>The benefits from a good study design can be negated by using poor   analytic technique. If the data are not analyzed properly, the results   of the study may be considered virtually worthless. Therefore, when   proposing a study, some consideration should be given to the method of   presenting how the hypotheses will be analyzed. Again, by demonstrating a   knowledge of the basic tenets of data analytic techniques, the   likelihood of obtaining a fundable priority score is increased. </p>
        <p><em>The Meaning of p </em></p>
        <p>A comprehensive discussion of probability theory is not possible   here. However, the following information will be provided to help avoid   confusion about the meaning of p, the value that is usually used to   determine whether the <a href="desgloss.html#Hypothesis">hypothesis</a> is supported from the results of the study. Some researchers have a   substantial misunderstanding of p, thinking that p is solely an index of   the magnitude of the <a href="desgloss.html#Relationship">relationship</a>. Actually, the <a href="desgloss.html#Test Statistic">test statistic</a> (e.g., t for the Student's <a href="desgloss.html#t-Test">t-test</a>, r for a Pearson <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a>, etc.) is an index of the size of the observed <a href="desgloss.html#Relationship">relationship</a>. p indicates how likely the observed <a href="desgloss.html#Relationship">relationship</a> could have been a result of chance alone. If a trial is conducted    several times, the results are likely to vary somewhat because of random   occurrences. For example, if a <a href="desgloss.html#Fair Coin">fair coin</a>—one   in which  heads and tails is equally likely on any given flip—is   flipped 100 times, &quot;heads&quot; would be expected to occur 50 times. In   practice however, obtaining 51 heads during a trial would not be   terribly surprising. The further the observed value is from the expected   value, the less likely the results can be attributed to chance.</p>
        <p>Usually an <a href="desgloss.html#Alpha">alpha</a>, the p-value at which a finding will be declared statistically <a href="desgloss.html#Significant">significant</a>,   is set at &lt;.05. This value indicates that the researcher is willing   to accept no greater than a 1 in 20 chance (5 chances in 100) that the   magnitude of the effect is from random fluctuation in the data; that is,   for every 20 statistical tests conducted one would be expected to   attain significance because of chance alone. Because 1) increasing the   number of subjects in the study increases the likelihood that the   population is not influenced by individuals who could be considered <a href="desgloss.html#Outliers">outliers</a> because 2) the sample is less likely to be atypical, a <a href="desgloss.html#Significant">significant</a> p value may be obtained with a very, very modest <a href="desgloss.html#Relationship">relationship</a> when  the sample consists of hundreds of observations. Note, however, poor research design, <a href="desgloss.html#Bias">bias</a>, or large amounts of error are not accounted for when calculating the <a href="desgloss.html#Test Statistic">test statistic</a> and deriving p.</p>
        <p>Researchers should also distinguish whether their statistical test will be for a <a href="desgloss.html#One-Tailed">one-tailed</a> <a href="desgloss.html#Hypothesis">hypothesis</a> or a <a href="desgloss.html#Two-Tailed">two-tailed</a> <a href="desgloss.html#Hypothesis">hypothesis</a>. A <a href="desgloss.html#One-Tailed">one-tailed</a> <a href="desgloss.html#Hypothesis">hypothesis</a> is one in which a particular <a href="desgloss.html#Relationship">relationship</a> is expected. For example:</p>
        <ul>
          <li>Drug A is expected to be more efficacious than drug A.</li>
          <li>Males are more likely to exhibit signs of drug abuse than females.</li>
          <li>Increasing the dosage of a given drug has a linear adverse effect on cognitive function.</li>
        </ul>
        <p>A <a href="desgloss.html#Two-Tailed">two-tailed</a> <a href="desgloss.html#Hypothesis">hypothesis</a> is one in which an effect is expected, but the direction of the <a href="desgloss.html#Relationship">relationship</a> is not known. <a href="desgloss.html#Two-Tailed">Two-tailed</a> hypotheses can be generated from the above examples:</p>
        <ul>
          <li>Drug A is expected to be different from drug A in terms of efficacy.</li>
          <li>Males and females are different in the signs of drug abuse that they exhibit.</li>
          <li>Increasing the dosage of a given drug has some type of effect (either adverse or enhancing effect on cognitive function).</li>
        </ul>
        <p>To achieve <a href="desgloss.html#Statistical Significance">statistical significance</a>, the magnitude of the <a href="desgloss.html#Relationship">relationship</a> does not have to be as large for a <a href="desgloss.html#One-Tailed">one-tailed</a> test as  it does for a <a href="desgloss.html#Two-Tailed">two-tailed</a> test. Therefore, <a href="desgloss.html#Two-Tailed">two-tailed</a> tests of the <a href="desgloss.html#Hypothesis">hypothesis</a> are usually considered more rigorous, and are often more encouraged in papers and grant proposals.</p>
        <p>Another major consideration when writing grant proposals is to avoid   the suggestion that data will be collected on hundreds of variables, and   a correlational matrix (e.g., one in which the <a href="desgloss.html#Relationship">relationship</a> of each variable to every other variable is estimated) will be generated. If <a href="desgloss.html#Alpha">alpha</a> is set at .05, one of twenty correlations will be found <a href="desgloss.html#Significant">significant</a> because of chance; if the correlational matrix contains hundreds of variables, dozens of <a href="desgloss.html#Significant">significant</a> results will be spuriously obtained. Therefore, such an analytic strategy is fraught with problems of identifying the &quot;truly&quot; <a href="desgloss.html#Significant">significant</a> variables from those that are &quot;chance&quot; <a href="desgloss.html#Significant">significant</a>. Investigators should always be specific about their <a href="desgloss.html#Hypothesis">hypothesis</a> and design the data analytic strategy accordingly; rarely should research be conducted on an atheoretical basis.</p>
        <p>However, sometimes a large number variables is expected to be related   to the variable of interest. The same problem exists in this case as in   the atheoretical example given above. Many variables may be identified   as significantly related to the <a href="desgloss.html#Outcome-Variable">outcome variable</a>, but only because of chance. Under such circumstances, either <a href="desgloss.html#Alpha">alpha</a> should be lowered to, say, .01 (meaning that only 1 finding in 100 statistical tests would be found <a href="desgloss.html#Significant">significant</a> as a result of chance), or more drastically, an adjustment to the   number of tests conducted can be applied. One of the most often used   adjustments is Bonferroni's measure in which <a href="desgloss.html#Alpha">alpha</a> is divided by the number of tests conducted. Therefore, if <a href="desgloss.html#Alpha">alpha</a> were set at .05  and twenty statistical tests were generated, a p-value   of .0025 would have to be achieved for a finding to be declared    statistically <a href="desgloss.html#Significant">significant</a>.   This type of adjustment may be useful in some areas of research,   however, by requiring relationships to be of such a large magnitude, <a href="desgloss.html#Type-II-Error">Type II error</a>s may occur, yielding a study in which no <a href="desgloss.html#Significant">significant</a> results are found.</p>
        <p><em>Primary Considerations</em></p>
        <p>One assumption that underlies many statistical tests, called <a href="desgloss.html#Parametric">parametric</a> tests, is that the data are distributed in a normal,  bell-shaped   curve. That is, relatively few observations are found at the extreme low   and the extreme high end of the scale with the highest percentage of   people clustering around the average. However, such is not always the   case; scores may often be skewed to either the high or low end of the   scale. The further that the distribution of the data depart from being <a href="desgloss.html#Parametric">parametric</a>, the less reliable the results from a <a href="desgloss.html#Parametric">parametric</a> test will be.</p>
        <p>For example, in some instances, <a href="desgloss.html#Outliers">outliers</a>—observations   with very high or very low values—may influence the mean of the data   when most of the scores follow a bell-shaped pattern. Skewed data may be   analyzed after the scores have been transformed to another scale (e.g.,   log linear, square root, or exponential). Also, <a href="desgloss.html#Parametric">parametric</a> analyses can be conducted with data that are not bell shaped after the   data have been ranked, with the highest observation receiving a &quot;1&quot; the   next highest a &quot;2&quot; and so on; of course, the opposite scoring may also   be performed—the lowest score receives a &quot;1&quot; the next lowest a &quot;2&quot; etc.   The decision to rank from low to high or from high to low will not   affect the <a href="desgloss.html#Test Statistic">test statistic</a>s and the resultant p-value. However, although usually successful, using one of the various types of <a href="desgloss.html#Transformations">transformations</a> may not be sufficient to &quot;bend&quot; the data back toward the underlying assumption of a normal distribution.</p>
        <p>The alternative to using transformed data is to utilize <a href="desgloss.html#Nonparametric">nonparametric</a> statistical tests. These are sometimes called  &quot;distribution free methods&quot; of analyzing data. For most <a href="desgloss.html#Parametric">parametric</a> statistical tests, there is a <a href="desgloss.html#Nonparametric">nonparametric</a> counterpart. One potential drawback is that <a href="desgloss.html#Nonparametric">nonparametric</a> tests are often less powerful (e.g., able to detect a <a href="desgloss.html#Significant">significant</a> result) than are their <a href="desgloss.html#Parametric">parametric</a> counterparts.</p>
        <p>However, the distribution of the data is not the only consideration   when selecting a statistical test. The temporal  nature of the   data—whether it is time series data—is also important; data from a <a href="desgloss.html#Cross-Sectional">cross-sectional</a> study should not be analyzed  in the same manner as data obtained over   time from multiple observations on the same individuals. Longitudinal   studies provide a good deal of information because the same subjects are   followed over time to identify any changes that may take place, an   example could be a study that assesses pre- and post-treatment status.   In this example, the data are paired and if the <a href="desgloss.html#Outcome-Variable">outcome variable</a> is <a href="desgloss.html#Continuous">continuous</a> and there is a grouping variable (e.g., <a href="desgloss.html#Cases">cases</a> receiving treatment versus <a href="desgloss.html#Controls">controls</a> receiving no treatment) a paired <a href="desgloss.html#t-Test">t-test</a> should be used to assess change.</p>
        <p>Equations used to calculate each test are not included for two   reasons. First, these equations can be found in any acceptable book on   statistics. Second, although the tests described below do not usually   require a great deal of computational effort, we assume the majority of   analyses in future studies will be computer generated. As many   investigators have already discovered computer assisted data analysis   provides the solutions to a variety of statistical tests accurately,   quickly, and relatively inexpensively. However, although the test   equations are not included here, some statistical terminology should be   reviewed.</p>
        <p><em>Advanced Data Analytic Techniques</em></p>
        <p>Usually in research involving human subjects, potential <a href="desgloss.html#Confounding-Variables">confounding variables</a> may unduly influence the results. <a href="desgloss.html#Confounding-Variables">Confounding variables</a> are those that are related to both the <a href="desgloss.html#Outcome-Variable">outcome variable</a> and the variable(s) hypothesized to influence (or merely be related to) the <a href="desgloss.html#Outcome-Variable">outcome variable</a>. For example, socioeconomic status <a href="desgloss.html#SES">(SES</a>) is often related to <a href="desgloss.html#Morbidity">morbidity</a> or treatment outcomes, and genders may differ in their adherence to treatment regimens. <a href="desgloss.html#Multivariate-Analyses">Multivariate analyses</a> can be used to partial out or <a href="desgloss.html#Control">control</a> for the effects of these possibly <a href="desgloss.html#Confounding-Variables">confounding variables</a>. Thus, an investigator may adjust for the effects of <a href="desgloss.html#SES">SES</a> and gender in a study of treatment efficacy in a multivariable model.   Because several variables can be entered into an equation at once, the   individual contribution of these variables to the <a href="desgloss.html#Outcome-Variable">outcome variable</a> can be assessed, and the collective <a href="desgloss.html#Relationship">relationship</a> of all of the variables in the equation to the <a href="desgloss.html#Outcome-Variable">outcome variable</a> can be estimated.</p>
        <p>In multivariate <a href="desgloss.html#Models">models</a>,   the outcome is usually defined as the dependent variable, and the   factors thought be associated with the outcome, including the potential   confounders, are known as the <a href="desgloss.html#Dependent-Variables">independent variables</a>. Researchers should not succumb to the propensity to think of the <a href="desgloss.html#Outcome-Variable">outcome variable</a> as actually being dependent on the <a href="desgloss.html#Dependent-Variables">independent variables</a>. These terms are often only used to distinguish the <a href="desgloss.html#Outcome-Variable">outcome variable</a> from the other variables in computer programs. The concept of <a href="desgloss.html#Dependence">dependence</a> and independence becomes muddles, particularly in <a href="desgloss.html#Cross-Sectional">cross-sectional</a> and retrospective studies.</p>
        <p>An example of an equation (in this case a <a href="desgloss.html#Case-Variable-Ratio">multiple regression</a>) of a multivariate analysis is: </p>
        <p>TREATMENT OUTCOME= CONSTANT + b1x1 + b2x2 + ... + bixi </p>
        <p>where Treatment OUTCOME is a value on a <a href="desgloss.html#Continuous">continuous</a> scale, such as a severity index; b is the slope for x and x is the  independent variable—in the example above, x1 could be <a href="desgloss.html#SES">SES</a>, x2 would be gender, and xi (i denotes the final in the series of variables) would contain the information about whether the <a href="desgloss.html#Subject">subject</a> were in treatment or not. The results would show the <a href="desgloss.html#Relationship">relationship</a> of each independent variable—adjusted for the influence of the other   variables in the independent variable list—with treatment outcome, and   the strength (usually noted as R or a Multiple R) of the combined <a href="desgloss.html#Relationship">relationship</a> of the <a href="desgloss.html#Dependent-Variables">independent variables</a> to the outcome. R is analogous to r in that squaring (e.g., to R2) that   value will yield an estimate of the variance explained by the model.   The variance explained may be easier to understand by  knowing if the   variance explained were removed, 1-R2 would remain in the <a href="desgloss.html#Outcome-Variable">outcome variable</a>. Thus, if R2=.20, removal of the variables in the equation, if possible, would leave an <a href="desgloss.html#Outcome-Variable">outcome variable</a> with only 80% of its original values.</p>
        <p>Unfortunately, some researchers will obtain data on many variables,   put them into an equation, and generate some  coefficients. Rather than   rely on an atheoretical strategy, almost always the selection of   variables to include in multivariable analyses should be made   judiciously because the results of the equation will depend on the   interrelationship of all the variables identified in the model. More   exploratory types of research may be  able to utilize a less theoretical   approach, however.</p>
        <p>In addition to controlling of the effects of <a href="desgloss.html#Confounding-Variables">confounding variables</a>, however, <a href="desgloss.html#Multivariate-Analyses">multivariate analyses</a> also allow the  development of parsimonious <a href="desgloss.html#Models">models</a>. That is, sometimes an independent variable may have a strong <a href="desgloss.html#Relationship">relationship</a> with the <a href="desgloss.html#Outcome-Variable">outcome variable</a> until a multivariable analysis is used when the <a href="desgloss.html#Relationship">relationship</a> falls to near zero. This phenomenon occurs when the amount of variance   explained by the independent variable in question is the same as that   explained by other variables in the equation.</p>
        <p>However as with all data analytic techniques, researchers should be   aware of the underlying assumptions, problems, or other considerations   associated with multivariate research. One factor is that of <a href="desgloss.html#Multicollinearity">multicollinearity</a> (or collinearity) among the <a href="desgloss.html#Dependent-Variables">independent variables</a>. When two or more of the <a href="desgloss.html#Dependent-Variables">independent variables</a> share essentially the same information, they are redundant to one   another, and the coefficients derived in a multivariate equation can   expected to be unstable. Examining the <a href="desgloss.html#Correlation">correlation</a> coefficients between the <a href="desgloss.html#Dependent-Variables">independent variables</a> may not provide the necessary evidence that <a href="desgloss.html#Colinearity">colinearity</a> may exist in the analysis because the information contained in one   variable may be also contained in two or more other variables in the   same equation—a possibility that would not be identified by looking at   Pearson <a href="desgloss.html#Correlation">correlation</a> coefficients. Most statistical packages contain programs that will yield an index of <a href="desgloss.html#Colinearity">colinearity</a>. One of the easiest indicators of <a href="desgloss.html#Colinearity">colinearity</a> to understand is <a href="desgloss.html#Tolerance">tolerance</a> which is computed by (1-R)2 where R is the multiple R of the individual   variable with all the other variables in the equation; thus low values   of <a href="desgloss.html#Tolerance">tolerance</a> indicate high <a href="desgloss.html#Colinearity">colinearity</a>. When the <a href="desgloss.html#Tolerance">tolerance</a> is about .20 for a given variable, other variables in the equation may &quot;flip&quot; signs from r to <a href="desgloss.html#Beta">Beta</a>. In other words, a variable may have a Pearson <a href="desgloss.html#Correlation">correlation</a> of r=.20 with the <a href="desgloss.html#Outcome-Variable">outcome variable</a>, but in an equation suffering from <a href="desgloss.html#Colinearity">colinearity</a>, have a <a href="desgloss.html#Beta">Beta</a>=-.30. Such results should not be considered &quot;real&quot; because on of the basic assumptions of <a href="desgloss.html#Multivariate-Analyses">multivariate analyses</a>—the assumption of &quot;independence&quot; among the <a href="desgloss.html#Dependent-Variables">independent variables</a>—is clearly violated.</p>
        <p>Another common problem that is easily avoided is the inclusion of too   many variables in the independent variable list for a given equation.   Having more than about twenty independent variable, the resultant   estimates may become unstable. The instability can be better illustrated   by considering the intercorrelations among the <a href="desgloss.html#Dependent-Variables">independent variables</a> are necessary to estimate the strength with which the variables are related to the <a href="desgloss.html#Outcome-Variable">outcome variable</a>; if an equation had fifty <a href="desgloss.html#Dependent-Variables">independent variables</a>, the <a href="desgloss.html#Correlation">correlation</a> matrix would be equal to 50 x 49 correlations. If 5% of these   correlations are a result of chance, it's easy to see that a high number   of random correlations could influence the equation estimates.</p>
        <p>However, note that if the sample is small, a similar problem may   occur. Therefore, the case (the number of subjects in the   analysis)/variable (the number of variables in the equation) ratio   should be kept high—about 10 to 1. This strategy rests on the assumption   some subjects will present atypical data for any given independent   variable—influencing the results of the entire equation. Thus, having   ample number of subjects in the equation while considering the number of   variables in the independent variable list makes sense.</p>
        <p><em>Final Considerations</em></p>
        <p>Collecting data from pilot studies is often very wise prior to submitting a grant application. Presumably, the results from the <a href="desgloss.html#Case-Variable-Ratio">pilot study</a> will support the investigator's arguments for conducting the larger   study. The financial cost of such studies is often nominal, and   collecting the data shows commitment on the part of the investigator. Of   course, because the <a href="desgloss.html#Ordinal Data">sample size</a> will undoubtedly be small, a more liberal <a href="desgloss.html#Alpha">alpha</a> can be used (e.g., .10 to .15, for example) for the data analysis.</p>
        <p>Throughout the development of the study design, the researcher should consider potentially <a href="desgloss.html#Confounding-Variables">confounding variables</a> and the means by which to manage such potential problems. Outcomes may   be influenced by factors such as age (particularly in children of   different ages when variations in developmental processes may influence   the responses), <a href="desgloss.html#Ethnicity">ethnicity</a>, gender, and <a href="desgloss.html#SES">SES</a>. Also, depending on the study, geographic differences among groups, <a href="desgloss.html#Cohort">cohort</a> effects, temporal variability in the data may affect the results.</p>
        <p>Most studies are designed to provide correlational as opposed to <a href="desgloss.html#Causal">causal</a> data. <a href="desgloss.html#Causal">Causal</a> relationships are difficult to prove.  Years ago Koch suggested the   following criteria for demonstrating that a bacteria was responsible for   causing a disease:</p>
        <ul>
          <li>the organism is present in each case</li>
          <li>the organism can be grown in pure culture</li>
          <li>the organism precipitates the disease in inoculated, susceptible animals</li>
          <li>the organism can be recovered from the animal and identified</li>
        </ul>
        <p>However, these criteria do not work well with of types of studies.   Typically, the following are needed to show that one  variable causes   another:</p>
        <ul>
          <li><a href="desgloss.html#Temporal-Relationship">temporal relationship</a> between cause and effect (the variable that is thought to cause the   factor of interest should precede the development of the outcome)</li>
          <li>strength of the <a href="desgloss.html#Association">association</a> (clinical, statistical) </li>
          <li><a href="desgloss.html#Dose-Response-Relationships">dose-response relationships</a></li>
          <li>reversible <a href="desgloss.html#Association">association</a> (if the <a href="desgloss.html#Causal">causal</a> factor is removed, the <a href="desgloss.html#Risk">risk</a> is reduced)</li>
          <li>consistency of findings across studies</li>
          <li>plausibility—do the results seem reasonable</li>
        </ul>
        <p><em>Parametric statistics</em></p>
        <p><a href="desgloss.html#Parametric">Parametric</a> tests are based on the assumption that the distribution of the data is   bell-shaped. When this assumption is  violated, analyzing the data with   another statistical technique should be considered. However, <a href="desgloss.html#Parametric">parametric</a> measures are often as efficient as their counterparts even when the   assumption about the distribution is violated. Of course, the more the   distribution departs from normality, the less appropriate the use of <a href="desgloss.html#Parametric">parametric</a> statistics becomes.</p>
        <p>The <a href="desgloss.html#t-Test">t-test</a> has a <a href="desgloss.html#Continuous">continuous</a> dependent variable and an independent variable that is <a href="desgloss.html#Categorical">categorical</a> with two levels. For  example, in a study of drug use, frequency of use   could be the dependent variable and gender (males versus females) as   the independent variable. The t statistic is based on the difference of   the groups' averages, the variability of the scores, and the <a href="desgloss.html#Ordinal Data">sample size</a>.</p>
        <p>When the mean scores of the groups have unequal variances as indicated by a comparison of the <a href="desgloss.html#Dose-Response-Relationships">standard deviation</a>s, the typical <a href="desgloss.html#t-Test">t-test</a> may yield unreliable results. In such a case, the t statistic should be   calculated as Satterthwaite suggested, which takes into account the   different variances of the two groups. When reporting the results of a <a href="desgloss.html#t-Test">t-test</a>, authors should usually note the respective means and <a href="desgloss.html#Dose-Response-Relationships">standard deviation</a>s of the two groups, the t statistic, the p-value.</p>
        <p>A <a href="desgloss.html#t-Test">t-test</a> cannot be employed when the <a href="desgloss.html#Categorical">categorical</a> variable has more than two levels. Instead, a one way Analysis of Variance <a href="desgloss.html#ANOVA">(ANOVA</a>) is used. Again the dependent variable is at least of ordinal scale the and the independent variable is <a href="desgloss.html#Categorical">categorical</a>. The <a href="desgloss.html#Test Statistic">test statistic</a> for the <a href="desgloss.html#ANOVA">ANOVA</a> is F. When only a two level <a href="desgloss.html#Categorical">categorical</a> variables is used in an <a href="desgloss.html#ANOVA">ANOVA</a>, F=t2, where t is the statistic from the <a href="desgloss.html#t-Test">t-test</a>.</p>
        <p>An <a href="desgloss.html#ANOVA">ANOVA</a> could be utilized in a study in which differences in drug use are   expected across three groups of subjects with each group coming from a   different high schools. As in the <a href="desgloss.html#t-Test">t-test</a>, if the <a href="desgloss.html#Dose-Response-Relationships">standard deviation</a>s are decidedly different across groups, the results of the <a href="desgloss.html#ANOVA">ANOVA</a> may not be valid. When this occurs, however, steps can be taken to ensure the results of the <a href="desgloss.html#ANOVA">ANOVA</a> are valid. For example, the dependent variable may be transformed to another scale (log e, square root, etc.), or the <a href="desgloss.html#Nonparametric">nonparametric</a> Kruskall-Wallis test, which is described below, should be used. The results of the <a href="desgloss.html#ANOVA">ANOVA</a> should include F, p, and usually the means and the <a href="desgloss.html#Dose-Response-Relationships">standard deviation</a>s of the groups.</p>
        <p>However, the results from the <a href="fdesgloss.html#ANOVA">ANOVA</a> only indicate that one of the group means differs from the overall mean   of the sample. Visual examination of the individual group means may   yield no clear answer about which of the means is different. Additional   tests may be conducted to determine which group(s) is significantly   different from the others. That is, mean frequency of drug use from the   youngest age group can be compared to those of two or more older groups.   Moreover, average scores from two groups can be combined to contrast   with the mean score of another group. These  tests include Sheff‚'s   multiple comparison procedure, Duncan's multiple range test, and Tukey's   studentized range test. Scheff‚'s is considered the least and Tukey's   the most conservative test of whether the mean scores of specific groups   are significantly different from one another. Before Sheff‚'s,   Duncan's, or Tukey's tests are conducted, however, the results from the <a href="desgloss.html#ANOVA">ANOVA</a> should be known, and the results should be <a href="desgloss.html#Significant">significant</a>. Otherwise, arbitrarily applying these tests increases the <a href="desgloss.html#Risk">risk</a> of finding an unacceptable number of chance relationships.</p>
        <p>Linear regression, also known as a Pearson <a href="desgloss.html#Correlation">correlation</a> or <a href="desgloss.html#Simple-Linear-Regression">simple linear regression</a>, is used to test the linear <a href="desgloss.html#Relationship">relationship</a> between two variables of at least ordinal scale. The dependent   variable is denoted as y and the independent variable is x. y is placed   on the vertical axis and x is on the horizontal axis.</p>
        <p>In an example, the <a href="desgloss.html#Association">association</a> between psychological depression and drug use could be tested with   linear regression. If the dependent variable increases as the   independent variable increases, a positive <a href="desgloss.html#Relationship">relationship</a> is said to exist. If the  dependent variable decreases as the independent variable increases, then a negative <a href="desgloss.html#Relationship">relationship</a> exists.</p>
        <p>r is the <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a> and the index for the magnitude of the <a href="desgloss.html#Relationship">relationship</a> between the dependent and <a href="desgloss.html#Dependent-Variables">independent variables</a>. Squaring r (yielding, of course, r2) produces an estimate of the amount of variance that can be accounted for by the observed <a href="desgloss.html#Relationship">relationship</a>.</p>
        <p>One potential problem with <a href="desgloss.html#Simple-Linear-Regression">simple linear regression</a> is that <a href="desgloss.html#Outliers">outliers</a> may influence the <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a>, especially in small samples. Generating a scatterplot of the two variables is usually advisable because <a href="desgloss.html#Outliers">outliers</a> can be easily spotted and nonlinear trends can be detected visually.</p>
        <p>If an outlier is apparently influencing the results, then the data   can be analyzed with a linear regression procedure after the variables   are <a href="desgloss.html#Type-II-Error">rank order</a>ed. This technique is less influenced by extreme values and yields the equivalent of a Spearman's <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a>. Kendall's <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a>, also a test using <a href="desgloss.html#Ranked-Data">ranked data</a> provides a solution similar to the Spearman statistic. These <a href="desgloss.html#Type-II-Error">rank order</a>ed tests are discussed later, but it should be pointed out here that they almost always more conservative than <a href="desgloss.html#Simple-Linear-Regression">simple linear regression</a>. However, to avoid losing data and biasing results, using a <a href="desgloss.html#Nonparametric">nonparametric</a> test is usually preferable to excluding the extreme values from a Pearson regression.</p>
        <p><em>Nonparametric Statistics</em></p>
        <p><a href="desgloss.html#Nonparametric">Nonparametric</a> statistics are not based on any assumption about the distribution of the data. <a href="desgloss.html#Nonparametric">Nonparametric</a> data analysis uses data that are either <a href="desgloss.html#Categorical">categorical</a> or, if the scale is <a href="desgloss.html#Continuous">continuous</a>, are ranked. <a href="desgloss.html#Ranked-Data">Ranked data</a>, in which the highest score is recorded as &quot;1,&quot; the next highest as &quot;2&quot;, etc., are considered ordinal. <a href="desgloss.html#Nonparametric">Nonparametric</a> statistical tests are usually more conservative than their <a href="desgloss.html#Parametric">parametric</a> counterparts, but again, the former has the advantage of not requiring the data to be in a bell shaped distribution.</p>
        <p>The <a href="desgloss.html#Chi-Square">chi-square</a> is usually utilized with an R (row) by C (column) table. For instance, <a href="desgloss.html#Chi-Square">chi-square</a> can be used to examine  the <a href="desgloss.html#Relationship">relationship</a> between gender by illicit drug use (yes/no). This statistic is based on   observed versus expected frequencies and is  an indication that at   least one value is different from expected. The value of <a href="desgloss.html#Chi-Square">chi-square</a> is equal to Z2, where Z is the standard score under the normal curve.</p>
        <p>A similar analysis, the <a href="desgloss.html#Chi-Square">chi-square</a> goodness of fit test can be used to determine if a variable has a   distribution comparable to the one expected. For example, the goodness   of fit test could be useful if a researcher expected a certain   distribution of excellent, good, medium, or bad results from an   intervention (for example: 80, 10, 5, and 5%, respectively) but had a   different outcome (95, 2, 2, and 1%).</p>
        <p>Investigators should note that when using <a href="desgloss.html#Chi-Square">chi-square</a>, when any cell in the table has no observations or when 20% of the cells have less than 5 subjects each, the <a href="desgloss.html#Chi-Square">chi-square</a> and resultant p value are of questionable <a href="desgloss.html#Validity">validity</a>. If one of the variables is ordinal, researchers should strongly consider another technique, such as a <a href="desgloss.html#t-Test">t-test</a> for analyzing the data.</p>
        <p>The Wilcoxon signed-rank test utilizes a ranked scale as the dependent variable and a two level <a href="desgloss.html#Categorical">categorical</a> scale as the  independent variable. The rank of one of the groups is   summed and the resultant value is compared to the expected sum of the   ranks. The results from a Wilcoxon test are the same as those of another <a href="desgloss.html#Nonparametric">nonparametric</a> test, the Mann-Whitney U test. These two tests are the <a href="desgloss.html#Nonparametric">nonparametric</a> counterpart of the <a href="desgloss.html#t-Test">t-test</a>. The <a href="desgloss.html#Test Statistic">test statistic</a> for the Wilcoxon test is Zc, also an estimate of the Z-value. For the Mann-Whitney, the U statistic is cited.</p>
        <p>If the independent variable has more than two groups, the   Kruskall-Wallis test should be used. Again, the dependent variable is   ranked and the independent variable has three or more categories. This   test is similar to the one way <a href="desgloss.html#ANOVA">ANOVA</a> and utilizes the H statistic.</p>
        <p>As noted earlier, when two variables are <a href="desgloss.html#Continuous">continuous</a>, a <a href="desgloss.html#Nonparametric">nonparametric</a> test of linear <a href="desgloss.html#Relationship">relationship</a> is Spearman's rs.  Spearman's regression is the mathematical equivalent   of Pearson's r when both variables are rank-ordered. When there are   many ties in either variable, rs can be misleading. In this case,   another <a href="desgloss.html#Nonparametric">nonparametric</a> regression, Kendall's ç, is best. Kendall's, however, is usually somewhat more conservative than Spearman's test.</p>
        <p><em>Tests for Paired Data</em></p>
        <p><a href="desgloss.html#Paired Data">Paired data</a> are obtained when subjects contribute more than one observation to the   study. The collection of pre- and post- drug intervention scores of   frequency of drug use is an example of <a href="desgloss.html#Paired Data">paired data</a>.   Each person in the data set contributed two values to the data set.   Except for the regressions which can be sued for time series analysis,   none of the above statistical tests are appropriate to <a href="desgloss.html#Analyze">analyze</a> <a href="desgloss.html#Paired Data">paired data</a> or any other data in which observations are not independent of one another.</p>
        <p>When an individual contributes to more than one observation the first   and second observations are implicitly related.  Individuals who have   high scores on any scale on one observation will usually have high   scores on the second observation. This phenomena is inconsistent with   the assumptions of standard statistical tests in which all observations   are independent.</p>
        <p>One of the most often used methods of testing for differences between <a href="desgloss.html#Paired Data">paired data</a> is the paired <a href="desgloss.html#t-Test">t-test</a>.   The mean difference (d) between the two scores (e.g., pre- and post-   intervention scores) is calculated and a t statistic is generated. The <a href="desgloss.html#Hypothesis">hypothesis</a> tested by the paired <a href="desgloss.html#t-Test">t-test</a> is that d is not equal to 0.</p>
        <p>Similar to the paired <a href="desgloss.html#t-Test">t-test</a>,   a variation of the Wilcoxon signed-rank test can be used to test for   differences among paired  observations. In the Wilcoxon matched pairs   test, differences are also noted between the matched pairs. However,   magnitude of the differences are ranked, the sum of the ranks between   the negative ranks is compared to the sum of the positive ranks yielding   the T statistic.</p>
        <p>The Friedman test is utilized when each observation provides more than two values for a variable. Technically, <a href="desgloss.html#Chi-Square">chi-square</a> r is the <a href="desgloss.html#Test Statistic">test statistic</a> which has a <a href="esgloss.htm#Chi-Square">chi-square</a> distribution when the group size and/or the number of scores are  not   small in number. However, when reporting the results of the Friedman   test the <a href="desgloss.html#Test Statistic">test statistic</a> is usually reported as <a href="desgloss.html#Chi-Square">chi-square</a>.</p>
      </div>
    </div></div>
<!-- End of Main Content -->
    
<!-- Last Updated -->
  <div class="update">Last Updated: July 23, 2012</div>
<!-- End of Last Updated -->
    
<!-- Footer -->
  <div id="foot-row1">
	<ul class="flist2">
    <li><a href="/index.html">Site Home</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/contact">Contact Us</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/policies/disclaimer">Disclaimer Policy</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/web/policies/accessibility">Accessibility</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/viewing-files">Viewing Files</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/web/policies/foia">FOIA</a></li>
    <li>|</li>
	<li><a href="/help.html">Help</a></li>
    <li>|</li>
    <li><a href="/site-map.html">Site Map</a></li>
  </ul>
	<br />

	<ul class="flist2">
    <li><a href="http://www.hhs.gov/">U.S. Department of Health and Human Services</a></li>
    <li>|</li>
  <li><a href="http://www.nih.gov/">National Institutes of Health</a></li>
  <li>|</li>
  <li><a href="http://www.cancer.gov/">National Cancer Institute</a></li>
  <li>|</li>
  <li><a href="http://www.usa.gov/">USA.gov</a></li>
  </ul>
  <div class="nih-tagline">NIH…Turning Discovery Into Health<sup>&#174;</sup></div>
	</div>
  </div>
<!-- End of Footer -->
</div>

<!-- Image map for head-row1 -->
<map name="Map" id="Map">
	<area shape="rect" coords="12,3,294,38" href="http://www.cancer.gov/" alt="National Cancer Institute" />
	<area shape="rect" coords="715,10,892,33" href="http://www.nih.gov" alt="U.S. National Institutes of Health" />
	<area shape="rect" coords="893,10,988,33" href="http://www.cancer.gov/" alt="National Cancer Institute" />
</map>
<!-- End of Image map for head-row1 -->

<!-- GA Code -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-25589575-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<!-- End of GA Code -->
<script language="JavaScript" type="text/javascript" src="/JS/Omniture/WA_DCCPS_PageLoad.js"></script>
</body>
</html>