<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <!-- meta tags--><meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--external style sheets--><link href="../style.css" type="text/css" rel="stylesheet" />
<title>DCCPS: Small Grants Program: About</title>
</head>
<body>
<div id="skipmenu">
<a href="#skip" class="skippy">Skip Navigation</a>
<a name="top"></a>
</div> <!-- end skipmenu -->
<div id="wrapper">
<!-- NCI Banner -->
  <div id="head-row1"><img src="../images/dccps-banner2.gif" alt="National Cancer Institute" width="1000" height="40" usemap="#Map" /></div>
<!-- end of NCI Banner -->

<!-- DCCPS Banner / Search Field -->
<div id="head-row2">
  <div id="left2"><a href="../index.html"><img src="../images/dccps-banner.gif" alt="Cancer Control &amp; Population Sciences Home - NCI's Bridge to Public Health Research, Practice and Policy" width="730" height="68" /></a></div>
  
  <div id="right2">
	<div id="north">
	<a href="http://twitter.com/NCICancerCtrl" title="Twitter">Twitter<img src="../images/twitter-icon.png" title="Twitter" alt="Twitter" width="20" height="20" /></a><a href="../exit_disclaimer.html"><img src="../images/Icon_External_Link.png" width="12" height="12" alt="exit disclaimer" /></a> <a href="../cr-communication-videos.html" title="Multimedia">Multimedia<img src="../images/media-icon.gif" title="Multimedia" alt="Multimedia" width="20" height="20" /></a></div>
    <form method="get" action="http://search2.google.cit.nih.gov/search" name="search">
	  <input type="hidden" name="site" value="DCCPS" />
	  <input type="hidden" name="client" value="DCCPS_frontend" />
	  <input type="hidden" name="proxystylesheet" value="DCCPS_frontend" />
  	  <input type="hidden" name="output" value="xml_no_dtd" />
	  <input type="hidden" name="filter" value="0" />
	  <input type="hidden" name="getfields" value="*" />
	  <label for="searchbox"><input id="searchbox" type="text" name="q" size="15" maxlength="255" class="htextf" value="Search" /></label>
	  <input type="image" src="../images/hbutton.gif"  class="hbutton" name="btnG" id="btnG" alt="Search" />
	</form><br class="clearfloat" />
    </div><br class="clearfloat" />
  </div>
<!-- End of DCCPS Banner / Search Field -->
	
<!-- Main Content -->
  <div id="mainarea">
  <!-- Left Column // DCCPS Link / Need Help Banner -->
  <div id="column-left1">
    <div id="dccps-link"><a href="../index.html">Cancer Control &amp; Population Sciences Home</a></div>
    <div class="rounded-border">
      <ul class="slist2">
        <li><a href="../funding.html" style="font-weight:bold;">Funding Opportunities Home</a></li>
      </ul>
    </div>
    <a href="http://www.cancer.gov/help"><img src="../images/help-tile.jpg" alt="Need Help? Contact us by phone (1-800-422-6237), Web, or e-mail" /></a> </div>
  <!-- End of Left Column // DCCPS Link / Need Help Banner -->
    
  <!-- Right Column // Page Content -->
    <div id="column-mid2">
	<a name="skip"></a>
      <div class="content">
        <h2>Step-By-Step Grant Help</h2>
        <h3>The Grant Writing Process - Design</h3>
        <h3>Instrument Selection</h3>
        <p><strong>Topics</strong></p>
        <ol>
          <li><a href="desgloss.html#Self-Report">Self-Report</a></li>
          <ul>
            <li>Survey interviews are often used to solicit information from subjects</li>
            <ul>
              <li><a href="desgloss.html#Telephone-Interview">Telephone interview</a></li>
              <li><a href="desgloss.html#Face-To-Face-Interview">Face-to-face interview</a></li>
              <li><a href="desgloss.html#Mailed-Questionnaires">Mailed questionnaires</a></li>
              <li>Advantages / disadvantages</li>
            </ul>
          </ul>
          <br />
          <li><a href="desgloss.html#Paper-Pencil-Measures">Paper / pencil measures</a> are popular to assess psychosocial or physical indices<br />
            Types of item</li>
          <ul>
            <li><a href="desgloss.html#Open-Ended-Items">Open-ended items</a></li>
            <ul>
              <li>Difficult to <a href="desgloss.html#Analyze">analyze</a></li>
              <li>May provide a wealth of information</li>
              <li>Identifying key words or phrases</li>
            </ul>
            <li>True / false</li>
            <li><a href="desgloss.html#Face-To-Face-Interview">Likert scales</a></li>
            <ul>
              <li>Leaving a neutral category</li>
              <li>Forcing an answer</li>
              <li>May lead to <a href="desgloss.html#Missing-Data">missing data</a></li>
              <li>May antagonize or alienate subjects</li>
              <li>Providing too many alternatives (e.g., &gt;7)</li>
              <li>Decreases <a href="desgloss.html#Readability">readability</a></li>
              <li>Increases time required to finish the instrument</li>
              <li>Responses should be in a consistent order (i.e., good to bad)</li>
              <li>The response categories should be pretty consistent throughout the instrument</li>
              <li>Responses should make sense</li>
              <li>Avoid vague response categories</li>
              <li>Increases in frequencies of engaging in a behavior should make   sense: not &quot;Do you have sex-- a) 5 times a day; b) 4 times a day; or c)   once a year?&quot;</li>
              <li>Properties of <a href="desgloss.html#Self-Report">self-report</a> measures</li>
              <li><a href="desgloss.html#Dimensionality">dimensionality</a> (how many dimensions are there to the <a href="desgloss.html#Construct">construct</a>?)</li>
              <li>How will items be scored? are they scored in that manner by most other investigators?</li>
              <li>Reliability <a href="desgloss.html#Validity">/validity</a> estimates</li>
            </ul>
          </ul>
          <br />
          The items should be constructed with a great deal of thought
  <ul>
    <li><a href="desgloss.html#Counterbalance">Counterbalance</a> to avoid problems with <a href="desgloss.html#Response-set">response set</a>s (also reduces some problems with fatigue)</li>
    <ul>
      <li>Items should be clear, simple</li>
      <li>Usually advisable to avoid slang or jargon</li>
      <li>Do not use double negatives</li>
    </ul>
    <li>Sensitive items are particularly  problematic:</li>
    <ul>
      <li>Increased <a href="desgloss.html#Risk">risk</a> of <a href="desgloss.html#Missing-Data">missing data</a></li>
      <li>Antagonizes some subjects</li>
      <li>Examples</li>
      <li>Income</li>
      <li>Alcohol &amp; drug use</li>
      <li>Sexual habits</li>
      <li>Beware of items that will elicit <a href="desgloss.html#Nominal-Data">socially desirable response</a>s vs. <a href="desgloss.html#Ethical-Concerns">ethical concerns</a> about the instrument's effect on the <a href="desgloss.html#Subject">subject</a></li>
      <li>Items that end up suggesting alternative behaviors</li>
      <li>Inadvertently identifies additional drugs</li>
      <li>Suggests behaviors that are socially deviant</li>
      <li>Induces anxiety</li>
    </ul>
    <li>The instrument should be constructed with the <a href="desgloss.html#Subject">subject</a> group in mind:</li>
    <ul>
      <li>no instrument can be used with all populations</li>
      <li>Translating one measure to another language (backtranslation)</li>
      <li>Identify <a href="desgloss.html#Research-Risks">research risks</a></li>
      <ul>
        <li type="disc">Some items may not be appropriate for the <a href="esgloss.html#Subject">subject</a> population</li>
        <li type="disc">Items written on a <a href="desgloss.html#Cognitive-Level">cognitive level</a> higher than most of the subjects will be functioning--an inventory   written for adult populations but used with a group of children</li>
      </ul>
      <li>Asking marital status of young adolescents</li>
      <li><a href="desgloss.html#Unobtrusive">Unobtrusive</a> or <a href="desgloss.html#Observational">observational</a></li>
    </ul>
    <li>Some examples of these types of studies are:</li>
    <ul>
      <li>Videotaping social situations</li>
      <li>In vivo observations <a href="desgloss.html#Unobtrusive">(unobtrusive</a>)</li>
      <li>Archival</li>
      <li>Ethnographic (participant observation)</li>
    </ul>
    <li>Some evidence for behaviors are</li>
    <ul>
      <li><a href="desgloss.html#Accretion">accretion</a></li>
      <li>Store displays</li>
      <li>Archival</li>
      <ul>
        <li type="disc">Market trends</li>
        <li type="disc">Hospital records</li>
        <li type="disc">Arrests</li>
      </ul>
      <li>Display <a href="desgloss.html#Cases">cases</a></li>
      <li>Observations in natural settings</li>
    </ul>
    <li>When developing <a href="desgloss.html#Observational">observational</a> measures, investigators may want to consider</li>
    <ul>
      <li>Ticking at specific time intervals</li>
      <li>Restricting categories (otherwise it will be too difficult to assess behaviors)</li>
      <li>If appropriate, ensure <a href="desgloss.html#Interrater-Reliability">interrater reliability</a></li>
      <li>Reports from others</li>
      <ul>
        <li type="disc"><a href="desgloss.html#Peer-Ratings">Peer ratings</a> often provide insight about subjects</li>
        <li type="disc">Supervisory or <a href="desgloss.html#Expert-Ratings">expert ratings</a> can also be helpful in gaining additional information about a <a href="desgloss.html#Subject">subject</a> measures from <a href="desgloss.html#Demographic-Data">demographic data</a> or administrative data</li>
        <li type="disc">Ratings of socioeconomic status are commonly related to <a href="desgloss.html#Outcome-Variable">outcome variable</a>s, particularly in studies of health or psychological well being</li>
        <li type="disc">Information on ecologic variables (census tract or group norm data) can provide data on large groups of people</li>
      </ul>
    </ul>
  </ul>
  <br />
  <li>Projective</li>
  <ul>
    <li>Asking the <a href="desgloss.html#Subject">subject</a> About what &quot;most people do&quot; or how &quot;most people feel&quot;</li>
    <li>Asking the <a href="desgloss.html#Subject">subject</a> To give an interpretation of an ambiguous item</li>
  </ul>
  <br />
          The advantage of using already constructed <a href="desgloss.html#Instruments">instruments</a> with evidence of reliability and <a href="desgloss.html#Validity">validity</a>
  <ul>
    <li>Issues related to reliability are</li>
    <ul>
      <li><a href="esgloss.html#Stability">Stability</a> / precision</li>
      <li>Reliability should never be assumed</li>
      <li>Types of reliability </li>
      <li><a href="desgloss.html#Internal-Consistency">Internal consistency</a></li>
      <li><a href="desgloss.html#Split-Half">Split half</a></li>
      <ul>
        <li type="disc">odd-even</li>
        <li type="disc">top-bottom</li>
      </ul>
      <li><a href="desgloss.html#Alpha">Alpha</a></li>
      <li><a href="desgloss.html#Kuder-Richardson">Kuder-Richardson</a> (for yes/no; true/false responses)</li>
      <li><a href="desgloss.html#Test-Retest">Test-retest</a></li>
      <li><a href="desgloss.html#Inter-Rater">Inter-rater</a></li>
      <ul>
        <li type="disc">All raters should be given <a href="desgloss.html#Consistent-Instructions">consistent instructions</a></li>
        <li type="disc">Note changes in raters' assessments over time <a href="desgloss.html#Maturation-effects">(maturation effects</a>) </li>
      </ul>
      <li><a href="desgloss.html#Expert-Ratings">Alternate form</a></li>
      <li>Increasing reliability</li>
      <ul>
        <li type="disc">Increase the number of items</li>
        <li type="disc"><a href="desgloss.html#Construct">Construct</a> scales with more response categories</li>
        <li type="disc">Maintain communications among raters</li>
        <li type="disc">The danger of reliability that is too high is pertinent to <a href="desgloss.html#Multidimensional-Concepts">multidimensional concepts</a> and poor <a href="desgloss.html#Sensitivity">sensitivity</a>, especially in studies of change</li>
      </ul>
    </ul>
    <li>If a scale has no evidence of <a href="desgloss.html#Validity">validity</a>, any findings from analyses that include the scale must be suspect. The types of <a href="desgloss.html#Validity">validity</a> are</li>
    <ul>
      <li><a href="desgloss.html#Construct">Construct</a></li>
      <li>content <a href="desgloss.html#Convergent">Convergent</a></li>
      <li><a href="desgloss.html#Discriminant">Discriminant</a></li>
      <li><a href="desgloss.html#External">External</a></li>
    </ul>
    <li>The importance of including relevant information on the reliability and <a href="desgloss.html#Validity">validity</a> of a scale—particularly if the scale is new or is not widely used—in a grant proposal</li>
  </ul>
  <br />
  <li><a href="desgloss.html#Sensitivity">Sensitivity</a> / <a href="desgloss.html#Specificity">Specificity</a></li>
  <ul>
    <li>If there is not an instrument that assesses the <a href="desgloss.html#Construct">construct</a> of interest, design an appropriate instrument—keep in mind this is a   time consuming process and requires meticulous work. The steps involved   in developing a new scale are:</li>
    <ul>
      <li>Examine <a href="desgloss.html#Previously-Constructed-Measures">previously constructed measures</a></li>
      <li>Define the <a href="desgloss.html#Components-Of-The-Concept">components of the concept</a> of interest by considering extreme examples or by reading pertinent current conceptual articles</li>
      <li>Identify items </li>
      <li><a href="desgloss.html#Pretest">Pretest</a> the instrument</li>
    </ul>
    <li>Utilize subjects who have characteristics similar to those of the <a href="desgloss.html#Population-Of-Interest">population of interest</a>.  Ask subjects</li>
    <ul>
      <li>If the items were clear</li>
      <li>If they had any problems</li>
      <li>If certain items bothered them</li>
      <li>About their thoughts while making responses</li>
      <li>If they had difficulty making responses</li>
      <li>Note the <a href="desgloss.html#Likert-Scales">range of responses</a></li>
      <li>Revise the instrument </li>
      <li><a href="desgloss.html#Pretest">Pretest</a> again</li>
      <li>Estimate reliability/collect evidence for <a href="desgloss.html#Validity">validity</a></li>
      <li>Be prepared to defend constructing the new scale</li>
    </ul>
  </ul>
  <br />
  <li><a href="desgloss.html#Factor-Analysis">Factor Analysis</a></li>
  <ul>
    <li>Obtain an adequate <a href="desgloss.html#Sample-Size">sample size</a> (~20 subjects per item) if <a href="desgloss.html#Factor-Analysis">factor analysis</a> is used</li>
    <li>This analysis is useful if the concept is multidimensional</li>
    <li>The methods of <a href="desgloss.html#Rotation">rotation</a> are:    		(1) Oblique <a href="desgloss.html#Rotation">rotation</a> and   		(2) Orthogonal <a href="desgloss.html#Rotation">rotation</a></li>
    <li>Conceptual problems arise when the empirical solution doesn't make sense</li>
    <li>Another conceptual difficulty is giving a subjective label to an <a href="desgloss.html#Empirically-Derived-Factor">empirically derived factor</a></li>
  </ul>
  <br />
  <li>Miscellaneous</li>
  <ul>
    <li>Never assume that an instrument measures the concept identified by the author of the test—always examine the items</li>
    <li>The test should be appropriate to the task at hand and to the population assessed</li>
    <li>Append copies of the measures to the grant application</li>
  </ul>
  <br />
  <li><a href="desgloss.html#Subject">Subject's</a> Habits of Reporting</li>
  <ul>
    <li>Some individuals will <a href="desgloss.html#Overreport">overreport</a> and  some will <a href="desgloss.html#Underreport">underreport</a></li>
    <li>In general, individuals at the extremes can be identified</li>
    <li>Bogus pipeline technique may help elicit more truthful responses from subjects</li>
  </ul>
        </ol>
        <p><strong>Discussion</strong></p>
        <p>Issues regarding the metric quality of the instrument are discussed elsewhere, but general guidelines are</p>
        <ol>
          <li>Items should be clear and presented in as simple language as possible. This strategy helps reduce <a href="desgloss.html#Missing-Values">missing values</a> from subjects whose reading skills may be limited. </li>
          <li>The items should not contain slang or jargon. Such a items are, at   best, useful for a short period of time until the slang or jargon become   anachronistic. c</li>
          <li>Double negatives in items are usually confusing and should be   avoided. An example is &quot;I don't use heroin&quot; with the possible response   categories of &quot;Always, Often, and Never&quot;.</li>
          <li>Finally, using an instrument that has drawn few valid criticisms is   clearly preferable to creating an instrument for ad hoc purposes. A   discussion about general types of <a href="desgloss.html#Data-Collection">data collection</a> <a href="desgloss.html#Instruments">instruments</a> can be collected is presented below.</li>
        </ol>
        <p><em>Self-report</em></p>
        <p>In this society, individuals are often asked to provide a great deal   of information about themselves for medical records, employment or   credit purposes, and a variety of other reasons. Asking subjects for   data may be the most direct means to collect reasonably accurate   information about the group of interest, but as will be discussed later,   these data should never be considered completely reliable or accurate.   Errors or <a href="desgloss.html#Bias">bias</a> in memory, an eagerness to please the researcher, nonverbal cues from   the experimenter, psychological affect, and environmental conditions may   influence response accuracy.</p>
        <p>Nonetheless, <a href="desgloss.html#Self-Report">self-report</a> data can be of sufficient quality to  test the hypotheses. <a href="desgloss.html#Self-Report">Self-report</a> data can be obtained utilizing a variety of techniques; however, regardless of the method of collecting <a href="desgloss.html#Self-Report">self-report</a> data, the items should never be ambiguous.</p>
        <p><a href="desgloss.html#Surveys">Surveys</a> are often conducted to yield a great deal of information on a large   number of people. The survey is useful to collect information on   demographics, attitudes, opinions, and certain behaviors. Items are   constructed in such a way that most responses will fit into one of   several predefined categories. Survey <a href="desgloss.html#Instruments">instruments</a> can be administered by mail or through periodicals, but the return rate   may be quite low, and the relatively few individuals who take the time   to respond may bear no resemblance to the <a href="desgloss.html#Population-Of-Interest">population of interest</a>.   Moreover, the package in which the instrument is delivered should   probably be one that is eye catching and attractive to prevent the   potential <a href="desgloss.html#Subject">subject</a> from throwing the package away with the circulars and other undesirable   mail. An obvious, well known survey that utilizes the mail is the   United States Census.</p>
        <p><a href="desgloss.html#Telephone-Interview">Telephone interview</a>s   can also be used and may yield a response rate that is higher than that   achieved by mail, possibly because most people find it more difficult   to hang up on a human voice than throw away mail that may be perceived   as &quot;junk&quot;. This trend may be reversing, however, with the increase in   telephone solicitations, particularly those that initially purport to be   from a research organization but end up as part of a sales campaign.   One drawback of <a href="desgloss.html#Telephone-Interview">telephone interview</a>s may be that not all of the <a href="desgloss.html#Target-Population">target population</a> has a phone, a condition that is especially true with new immigrants or   any minority of which a large percentage is impoverished. With such   populations, a <a href="desgloss.html#Biased">biased</a> sample will undoubtedly be obtained because only those who are affluent enough to afford a telephone will be contacted.</p>
        <p>Using an interviewer may increase response rates because 1) denying a   human face some degree of interaction may be more difficult than   hanging up on a <a href="desgloss.html#Telephone-Interview">telephone interview</a>er and 2) identification can be shown to the potential respondent to alleviate the <a href="desgloss.html#Subject">subject</a>'s anxiety about being subjected to a sales pitch. However, the cost per <a href="desgloss.html#Subject">subject</a> may be substantially higher using interviewers because the time lost when a potential <a href="desgloss.html#Subject">subject</a> is not home is usually increased over that of merely dialing another   telephone number. With a good deal of apparent success, the National   Health Interview <a href="desgloss.html#Surveys">Surveys</a> have used interviewers to obtain data from subjects.</p>
        <p><a href="desgloss.html#Self-Report">Self-report</a> data are often collected on multiple item paper/pencil <a href="desgloss.html#Instruments">instruments</a>,   although the items may also be administered by an interviewer. These   types of measures are popular for gathering data about physical   symptoms, psychologic factors, and a variety of other constructs. These   measures contain several items that are focused on one or more   constructs.</p>
        <p>Similar to the true/false format, responses can be obtained in a   Likert format. Such data is usually on an ordinal scale (e.g., ranked   from &quot;good&quot; to &quot;bad&quot;, &quot;best&quot; to &quot;worst&quot;, &quot;agree&quot; to &quot;disagree&quot; or   &quot;always&quot; to &quot;never&quot;). The items usually have five or seven response   possibilities. Having an odd number of response categories can be   desirable because a neutral response can fit neatly into the ordinal   classification (e.g., &quot;agree,&quot; &quot;neither agree nor disagree,&quot; or   &quot;disagree&quot;). A neutral response can sometimes reduce <a href="desgloss.html#Missing-Data">missing data</a> from subjects who are not willing to commit to an answer one way or   another. Note, however, that having too many categories may increase the   amount of time that a <a href="desgloss.html#Subject">subject</a> needs to respond to a series of items because of decreased <a href="desgloss.html#Readability">readability</a> and an increase in the amount of time required to consider the answer.</p>
        <p>Creating response categories can be difficult and requires some   thought if a previously published instrument is not being used. Again,   researchers are strongly encouraged to use measures that have been shown   valuable in other studies and to use scales that have counterbalanced   the items. However, occasionally a scale can be improved—if much   consideration has been given to changing the <a href="desgloss.html#Response-set">response set</a> of a previously used measure. Of course, any modifications to a scale   will have to be justified in the grant application or the article   describing the results of the study. An example for improving a scale is   to avoid vague or conflicting response categories such as including the   response possibilities:  &quot;often&quot; and &quot;most of the time.&quot; Also, the   categories should be at least roughly linear and make some type of sense   (e.g., an item such as &quot;Do you have sex: 1) 5 times a day; 2) 4 times a   day; 3) once a week; or 4) once a year&quot; will yield data of questionable   quality because most people will choose either &quot;1&quot; or &quot;2&quot;; response &quot;4&quot;   will rarely be chosen, but also note the lack of a &quot;never&quot; category,   which may draw some responses). The above example also shows that items   should have some discriminatory ability; having an item to which the   entire sample selects the same response provides data of little use.</p>
        <p>Generally, Likert items are more reliable or stable than true/false   items because any error in response is more dramatic when the response   goes from one category to the opposite category, such as, from &quot;true&quot; to   &quot;false.&quot;</p>
        <p><em>Other considerations</em></p>
        <p>Regardless of the format, additional factors should be considered   when selecting the instrument that will be used in the  study. First,   delicate items are sometimes difficult to present in such a way that   subjects will not feel antagonized and thus provide <a href="desgloss.html#Missing-Data">missing data</a>; too much <a href="desgloss.html#Missing-Data">missing data</a> will adversely influence the quality of the study. Subjects will not   always feel obliged to respond honestly about their income, particularly   in very high or very low income families; their sexual activities or   preferences; their use of alcohol, especially if they abuse alcohol; or   their illicit activities. Consequently, certain groups of subjects may   provide no or, perhaps worse, erroneous information to sensitive items.   Poor quality data can sometimes be reduced by convincing the <a href="desgloss.html#Subject">subject</a> that the data will be confidential. Also, placing  sensitive items   toward the end of the instrument or interview will sometimes give the <a href="desgloss.html#Subject">subject</a> a chance to become more relaxed while data are being obtained,   presumably increasing the possibility that such information will be more   accurately obtained.</p>
        <p>On a more ethical note, researchers should be concerned about the effect of the measure on the <a href="desgloss.html#Subject">subject</a>'s   later behaviors or level of anxiety. For instance, administering a drug   questionnaire to a pre-adolescent population may cause some problems if   a list of easily obtainable products that induce mood altering states   is contained in the instrument. Such a measure may well provide   encouragement to kids who would normally have not thought to abuse one   or more products included on the list. Also, some subjects may become   highly anxious when presented with certain items, such as those related   to the recent death of a spouse or child.</p>
        <p>Lastly, the instrument should be administered with the <a href="desgloss.html#Subject">subject</a> group in mind. No <a href="desgloss.html#Self-Report">self-report</a> measure exists that can be used with all populations. Measures that are   standardized with one population may either have to be translated, or   converted to an appropriate educational level. Moreover, some items may   reflect concepts that are offensive or that are not understood by the <a href="desgloss.html#Population-Of-Interest">population of interest</a>.</p>
        <p><em>Unobtrusive and Observational Methods</em></p>
        <p><a href="desgloss.html#Self-Report">Self-report</a> measures have some drawbacks. These include the possibility of   providing subjects with a menu of possible deviant behaviors from a   list; and often limiting with a set of categories to which the <a href="desgloss.html#Subject">subject</a> is forced to respond, thereby limiting the <a href="desgloss.html#Likert-Scales">range of responses</a>.   Another problem is that subjects will often try to guess what the   researcher is studying and will respond accordingly, rather than   responding with a valid answer. Alternative methods of collecting data   is either to behavior in either natural or contrived settings or to   collect data from sources other than the <a href="desgloss.html#Subject">subject</a>. Of course, the advantage of collecting these type of data is that errors or biases in the <a href="desgloss.html#Subject">subject</a>'s   responses are not possible. The disadvantages are that the settings are   difficult to capture or set up, or the desired data may not be   available.</p>
        <p>In some studies, investigators may videotape (or use audiotapes of)   subjects in certain situations, and the tape would  be reviewed later by   raters who would watch for a predetermined set of behaviors.   Videotaping a controlled event can be a good strategy, because <a href="desgloss.html#Interrater-Reliability">interrater reliability</a> (described elsewhere) can be accomplished by collecting data from   multiple raters on exactly the same videotape. If all raters have been   given similar training, dissimilarities in the data among raters would   not be expected. Further, videotapes can be reviewed many times and   particular moments stopped or replayed for instructional or evaluative   purposes. Observing behavior<em> in vivo</em> is usually much more   difficult because data is more easily lost through a lapse in the   attention of the rater or some other source of error. If such is the   case, the event obviously cannot be replayed.</p>
        <p>Of course, when data are collected from observed behavior, raters are   required. A predetermined list of behaviors is usually generated prior   to the study, and raters are asked to determine 1) whether a behavior   occurred, 2) when it occurred, and/or 3) how long the behavior lasted.   Because rating the number of behaviors over time can be difficult, often   these raters are only asked to tick behaviors on the list that are   currently occurring at some time mark (e.g., at every 5 or 10 minutes).</p>
        <p>Other <a href="desgloss.html#Unobtrusive">unobtrusive</a> sources of information are archival sources. That is, data have been   collected for purposes other than the proposed study. Medical and   hospital records are often used in research, for example. Public records   such as marriage licenses, arrest records, death certificates, and   court records may also be used to assess trends in a certain population.   Information such as market variability may also be obtained from   publications such as the popular press (e.g., newspapers) or trade   journals. If data are being collected from a study of community trends   rather than individuals, information can be obtained from census tract   records which will contain a variety of <a href="desgloss.html#Demographic-Data">demographic data</a> such as the breakdown of <a href="desgloss.html#Ethnicity">ethnicity</a>,   income, and age of the population. Information about an individual may   also be obtained, usually with that person's permission, from   administrative files such as annual progress reports, medical insurance   claims, etc.</p>
        <p>There are some problems with using <a href="desgloss.html#Archival-Data">archival data</a>.   First, the data may not be of the highest quality.  Investigators must   keep in mind that the data were not collected for use in any particular   study and some data were not intended for use in research. Researchers   may find that their archival sources are unsuitably incomplete or   generally inaccurate. Moreover, because much of the <a href="desgloss.html#Archival-Data">archival data</a> might have been collected years in the past, it may not include   information on variables that are of interest in research conducted more   recently. Therefore, a database may be generated that does not include   information on variables that have since been found to be important in   the proposed study.</p>
        <p>Under certain circumstances, other methods of <a href="desgloss.html#Unobtrusive">unobtrusive</a> observation may be of use to an investigator. For example, in some studies <a href="desgloss.html#Accretion">accretion</a>—the   accumulation of the factor of interest—may be used to identify popular   hangouts (examining the amount of litter that has accumulated in a given   area). Examining store displays may provide helpful data, too. For   example, the interest in cigarette smoking may be indicated by the   amount of space allotted to carrying cigarettes in a retail store.</p>
        <p>One means of obtaining data about a certain individual is to ask   other individuals about that person. Peers may provide data that may not   be consistent with that which the individual would give; thus peers may   provide data about a person that would otherwise not be available.   Information can also be gathered from experts (e.g., clinicians) or from   supervisors' ratings. These ratings may be used as a means by which the <a href="desgloss.html#Validity">validity</a> of the <a href="desgloss.html#Self-Report">self-report</a> <a href="desgloss.html#Instruments">instruments</a> can be assessed.</p>
        <p><em>Issues regarding the integrity of the scale</em></p>
        <p>The development of a new measure that will be accepted by the   research community is usually laborious and difficult. It is the   responsibility of critics to take every opportunity to challenge whether   the scale has adequate psychometric properties, and critics are more   than happy to do so. Every instrument must demonstrate reliability and <a href="desgloss.html#Validity">validity</a>.   When writing a grant proposal or a manuscript that will be submitted to   a peer reviewed journal, the investigator should offer ample evidence   that the measure has desirable characteristics (e.g., reliability and <a href="desgloss.html#Validity">validity</a>). If the measures have no record of reliability or <a href="desgloss.html#Validity">validity</a>, the proposal will almost certainly be either disapproved or assigned an approval score that would not likely be funded.</p>
        <p><em>Reliability</em></p>
        <p>Reliability can be described as <a href="desgloss.html#Stability">stability</a>,   consistency, or precision, and may be demonstrated in different ways.   The method of showing reliability depends on the type of instrument and   the intent of the researcher. If a measure is used that contains several   items to measure one <a href="desgloss.html#Construct">construct</a> and the items are ordinal having more than two levels, such as in a Likert scale, <a href="desgloss.html#Internal-Consistency">internal consistency</a> is often cited as evidence of reliability. The most often cited index of <a href="desgloss.html#Internal-Consistency">internal consistency</a> is Cronbach's <a href="desgloss.html#Alpha">alpha</a>. Before an explanation of <a href="desgloss.html#Alpha">alpha</a> can be made, however, some background about <a href="desgloss.html#Internal-Consistency">internal consistency</a> should be given. An obvious assumption of having a multiple item instrument is that the items  are all related to the same <a href="desgloss.html#Construct">construct</a>;   therefore, the items should be highly correlated with one another. An   early method of  assessing the homogeneity among items was to split  the   scale arbitrarily into two halves—very often the items appearing in the   first half were grouped against the last half of the items (called   top/bottom reliability); sometimes odd numbered items were grouped   against the even numbered items (called odd/even reliability). Scores   were generated for each group of items by summing the values of the   items contain in that particular group. A <a href="desgloss.html#Correlation">correlation</a> <a href="desgloss.html#Coefficient">coefficient</a> was obtained to estimate the strength of the <a href="desgloss.html#Relationship">relationship</a> between the two groups of items. High correlations, of course,   indicated that the two groups of items were highly related. This finding   provided evidence of <a href="desgloss.html#Internal-Consistency">internal consistency</a>, and generally was called <a href="desgloss.html#Split-Half">split-half</a> reliability. Analogous to Cronbach's <a href="desgloss.html#Alpha">alpha</a> but used with items that have dichotomous response categories (yes/no or true/false) is the <a href="desgloss.html#Kuder-Richardson">Kuder-Richardson</a> test for <a href="desgloss.html#Internal-Consistency">internal consistency</a>.</p>
        <p>When <a href="desgloss.html#Stability">stability</a> over time is of interest and the scale is at least ordinal, <a href="desgloss.html#Test-Retest">test-retest</a> reliability should be demonstrated. <a href="desgloss.html#Test-Retest">Test-retest</a> reliability is shown when the scores of the instrument correlate highly   with a later, readministration of the same measure. Such evidence for   reliability is especially important if a measure is used to classify   individuals as having some type of pathology or as eligible for certain   social programs. For example, multiple administrations over time of an   instrument that identifies individuals as having drug or alcohol   problems should yield similar results at each assessment. An instrument   that would classify individuals as drug dependent half of the time would   be of little diagnostic use and would not have much application in   treatment programs. Of course, a primary assumption of <a href="desgloss.html#Test-Retest">test-retest</a> reliability is that no intervening event has taken place that would   affect subjects' responses. Another assumption is that the interval of   time between the test and the retest should be meaningful and long   enough that <a href="desgloss.html#Bias">bias</a> from remembering the initial responses is unlikely.</p>
        <p>The above indices of reliability are not appropriate for <a href="desgloss.html#Data-Collection">data collection</a> <a href="desgloss.html#Instruments">instruments</a> used in certain types of research. When  obtaining any type of   subjective data, raters often must make some judgment about whether an   event or behavior has occurred or whether some physical or psychological   condition exists. Because no evidence would exist that a single rater   would be in agreement with other trained raters, use of multiple raters   is usually desirable to provide evidence for the ratings being free of <a href="desgloss.html#Bias">bias</a> from the individual rater. To utilize multiple raters, all raters must   receive the same instructions and they must be consistent with one   another in their evaluations; therefore, <a href="desgloss.html#Inter-Rater">inter-rater</a> reliability should be demonstrated. To show <a href="desgloss.html#Inter-Rater">inter-rater</a> reliability, the evaluations of two individuals who have rated same   occurrences or  records are compared; high agreement indicates good   reliability.  If several raters are used, each may be compared to one   another or to a single &quot;authoritative&quot; rater, often the principal   investigator. One potential problem of using raters is the effects of   maturation of the rater. Over time the person becomes more and more   competent. Because data that are rated toward the end of the project may   not be consistent with that rated early in the project, some   consideration should be given to having each rater review some sources   of information such as videotapes that were evaluated early. Of course, a   poor <a href="desgloss.html#Correlation">correlation</a> between the early ratings and the later ratings may indicate that all of the records should be re-evaluated.</p>
        <p>Obtaining an acceptable level of reliability depends on many factors besides the homogeneity of the items. For example, <a href="desgloss.html#Face-To-Face-Interview">Likert scales</a> are usually more reliable than scales with dichotomous response   categories. Also, maintaining a good line of communications among raters   will increase reliability because certain problems can usually be   resolved quickly and the entire group of raters is immediately aware of   the problems and resolutions. However, certain issues regarding   reliability are more difficult.</p>
        <p>For instance, the reliability of a scale can be increased by adding   items. However, whereas the score may be found to be more stable, the   additional items may not be conceptually consistent with the <a href="desgloss.html#Construct">construct</a> of interest. At any rate, sometimes very high reliability (e.g., <a href="desgloss.html#Alpha">alpha</a> &gt; .90 or a <a href="desgloss.html#Test-Retest">test-retest</a> of &gt; .90) is not necessarily desirable. If scores are too stable   over time, the instrument cannot be very sensitive, and therefore   moderate amounts or change over time cannot be assessed. Similarly, most   constructs should be considered multidimensional, and clusters of a few   items should be used to assess different domains of the <a href="desgloss.html#Construct">construct</a>.   The scores of these subgroups of items would be expected to correlate   highly with the total score, but not necessarily highly with the   subscores from the other groups of items. In such a case, <a href="desgloss.html#Alpha">alpha</a> will tend to be lower than a <a href="desgloss.html#Construct">construct</a> that contains more homogeneous items, although presumably <a href="desgloss.html#Test-Retest">test-retest</a> reliability will be high.</p>
        <p><em>Validity</em></p>
        <p><a href="desgloss.html#Validity">Validity</a> is the ability of the scale to measure the concept of interest. Consequently, <a href="desgloss.html#Validity">validity</a> is the most important concept in assessing the efficacy of a measure. The first guideline of <a href="desgloss.html#Validity">validity</a> is that <a href="desgloss.html#Validity">validity</a> cannot exist without reliability. The second premise is that for a   scale to be valid, items for the scale must have come from the universe   of all items that comprise the concept of interest.</p>
        <p>Evidence for <a href="desgloss.html#Construct">construct</a> <a href="desgloss.html#Validity">validity</a> is almost always indirect. <a href="desgloss.html#Construct">Construct</a> <a href="desgloss.html#Validity">validity</a> is the evidence that the scale measures the  concept that the   investigators purport it measures—which is also the basic definition of <a href="desgloss.html#Validity">validity</a>. <a href="desgloss.html#Construct">Construct</a> <a href="desgloss.html#Validity">validity</a> can never be proven, really, but satisfying evidence for the other types of <a href="desgloss.html#Validity">validity</a> will indirectly indicate that <a href="desgloss.html#Construct">construct</a> <a href="desgloss.html#Validity">validity</a> exists.</p>
        <p>More empirical evidence can be found for <a href="desgloss.html#Convergent">convergent</a> <a href="desgloss.html#Validity">validity</a>, and therefore this form of <a href="desgloss.html#Validity">validity</a> is usually easier to  demonstrate. For <a href="desgloss.html#Convergent">convergent</a> <a href="desgloss.html#Validity">validity</a>,   a scale should be highly correlated with other scales that purport to   measure either the same or, to a lesser extent, a similar <a href="desgloss.html#Construct">construct</a>.   Thus, measures of anxiety should be correlated with other existing   measures of anxiety, and to a lesser extent with measures of other types   of psychological affect such as depression. Evidence for <a href="desgloss.html#Convergent">convergent</a> <a href="desgloss.html#Validity">validity</a> is also obtained when the measure is inversely correlated with <a href="desgloss.html#Instruments">instruments</a> that assess the opposite <a href="desgloss.html#Construct">construct</a>.   A scale of anxiety should be inversely related to a measure of life   satisfaction because the two scales are conceptually opposite.</p>
        <p>When a measure has virtually no <a href="desgloss.html#Correlation">correlation</a> with a concept (that is, not conceptually related or even diametrically opposed to it), divergent <a href="desgloss.html#Validity">validity</a> has been demonstrated. An example would be that scores from a measure   of Type A behavior and anxiety are not be conceptually linked, and these   scales would not be expected to correlate highly with one another.</p>
        <p>Also more easily demonstrated than some of the other types of <a href="desgloss.html#Validity">validity</a> is <a href="desgloss.html#Discriminant">discriminant</a> <a href="desgloss.html#Validity">validity</a>. <a href="desgloss.html#Validity">validity</a> is shown when the measure can, with reasonable accuracy, identify subjects as having the condition of interest.</p>
        <p>Investigators who have created a new scale will usually try to demonstrate the criterion <a href="desgloss.html#Validity">validity</a> of the measure. This form of <a href="desgloss.html#Validity">validity</a> is demonstrated when the scale agrees with some other form of   evaluation. For example, experts may be asked to classify a group of   subjects as having a condition or not having the condition, and the   scale should agree with the classifications by the experts. For a   measure of anxiety, psychiatrists or psychologists may be asked to   evaluate a group of patients to categorize those with anxiety disorders;   the group identified as having some type of anxiety disorder should   have higher scores on the measure that is being assessed than the   individuals rated as not having an anxiety disorder.</p>
        <p>Face <a href="desgloss.html#Validity">validity</a> is the least substantial and lowest form of <a href="desgloss.html#Validity">validity</a>.   Here, the items only have to appear conceptually linked  to the concept   of interest. An example of items assessing age that have face <a href="desgloss.html#Validity">validity</a> include &quot;How old are you?&quot; and/or &quot;When were you born?&quot;</p>
        <p><em>Further Considerations</em></p>
        <p>Often another set of issues regarding a measure, particularly those that are to have diagnostic value, is that of <a href="desgloss.html#Sensitivity">sensitivity</a> and <a href="desgloss.html#Specificity">specificity</a>. <a href="desgloss.html#Sensitivity">Sensitivity</a> refers to the instrument's ability to identify individuals who have a   certain characteristic, such as a particular psychopathology or a   physical illness. <a href="desgloss.html#Specificity">Specificity</a> is the ability to classify accurately only those individuals with the   characteristics of interest. For an example, assume that 20% of a sample   coming to a clinic has a drug abuse problem. Rather than using lengthy   and more costly interviews to classify individuals, an investigator   decides to create a screening instrument that can be administered   quickly and inexpensively to identify people with drug problems.   Technically and in the most blatantly ludicrous scenario, the researcher   could choose the items at random—regardless of the content of the   items—administer the test to the sample, and declare that any score on   the test indicates a possible drug dependency. Such a scale would have   100% <a href="desgloss.html#Sensitivity">sensitivity</a>—all of the drug abusers would be correctly classified—but very poor <a href="desgloss.html#Specificity">specificity</a>, with none of the nonabusers identified.</p>
        <p>Of course, the researcher could correctly identify 100% of the   nonabusers and 80% of the entire sample if any score on the test was   proposed to indicate no drug abuse problem. Neither strategy in the   above example is recommended; yet, these types of problems exist when an   investigator tries to develop an instrument with acceptable levels of <a href="desgloss.html#Sensitivity">sensitivity</a> and <a href="desgloss.html#Specificity">specificity</a>. As <a href="desgloss.html#Sensitivity">sensitivity</a> increases, <a href="desgloss.html#Specificity">specificity</a> almost always decreases because of variability across individuals on   the items that are used to make the classifications. That is, some   individuals who are normal will score high enough on an instrument to be   incorrectly classified; other people who should be classified as having   a certain characteristics will score low enough that they will be   classified as &quot;normal.&quot;  Typically when such a scale is developed, <a href="desgloss.html#Sensitivity">sensitivity</a> and <a href="desgloss.html#Specificity">specificity</a> are demonstrated with an <a href="desgloss.html#External">external</a> criterion. That is, some other information is used to assess how well   the scale performs. In the above example of drug abuse, an experienced   clinician might be used to classify individuals after an interview, or   archival records such as arrest records, employment history, or <a href="desgloss.html#Self-Report">self-report</a>ed drug involvement might be used to evaluate the efficacy of the proposed scale.</p>
        <p><em>Scale construction</em></p>
        <p>Despite the fact that as many <a href="desgloss.html#Instruments">instruments</a> have been developed as McDonalds has sold hamburgers, sometimes an   investigator discovers a concept that has been poorly measured or   completely ignored, and of course, the concept in question is always   vital to the investigator's <a href="desgloss.html#Hypothesis">hypothesis</a>.   Outlined below are the steps or procedures that should be taken when   developing an instrument. These steps are particularly pertinent to   constructing a multi-item <a href="desgloss.html#Self-Report">self-report</a> paper/pencil instrument, but most may be generalizable to building other types of <a href="desgloss.html#Instruments">instruments</a>.</p>
        <p>Before continuing with this discussion, however, the reader is warned   that designing an instrument that will be acceptable for use in   research is a time consuming process and requires meticulous work.   Almost always, researchers should try to use scales already developed to   avoid any potential problems during review of a submitted proposal or a   manuscript. Moreover, when a new scale needs to be developed,   investigators are strongly advised to consult with  someone who has   experience in designing <a href="desgloss.html#Instruments">instruments</a>.</p>
        <p>The first step is to identify relevant and current conceptual   articles and obtain copies of similar previously constructed  measures.   The conceptual articles will likely provide a good starting point for   identifying the <a href="desgloss.html#Components-Of-The-Concept">components of the concept</a> of interest. The <a href="desgloss.html#Instruments">instruments</a> designed by previous investigators should measure either similar,   purportedly the same, or directly opposite constructs, and therefore may   be a source of items for the new scale. By examining the papers that   report the development of these measures, various concerns,   considerations, and approaches may be identified that were contemplated   or anticipated by the present investigator. Typically a good research   librarian is invaluable in locating references that contain pertinent   scales. Some suggestions about resources that may be useful to begin the    search, however, are using Mental Measurements Yearbook and   Psychological Tests in Print and running computer literature searches in   the appropriate databases such as Psychological Abstracts and   Sociological Abstracts.</p>
        <p>Once the accomplishments of past researchers have been taken into   consideration, a concise definition of the concept should be devised.   Once defined, an exhaustive list of items that are germane to the   concept should be developed. Usually, concepts are multidimensional, and   having a set of items that reflect the different dimensions is almost   always desirable. Using a panel of experts who are knowledgeable about   the concept may be used to add or discard redundant or unnecessary   items. In addition to relying on the expert panel to deselect items,   unneeded or useless items may be identified in the <a href="desgloss.html#Pretest">pretest</a>.</p>
        <p>For the <a href="desgloss.html#Pretest">pretest</a>, a relatively small sample (n~20) of subjects who have characteristics similar to the <a href="desgloss.html#Population-Of-Interest">population of interest</a> should be obtained. The list of items identified through the above   processes should be administered. When the subjects have finished, each   should be asked to evaluate the instrument. Some of the questions might   include:</p>
        <ul>
          <li>Were the items clear?</li>
          <li>Were there any problems understanding the items?</li>
          <li>Are the response categories appropriate?</li>
          <li>Did any items cause anxiety, or were some items 'bothersome'?</li>
          <li>What thoughts occurred when filling out the instrument?</li>
          <li>Was there any difficulty in making the required responses?</li>
        </ul>
        <p>Additionally, subjects should be encouraged to express any other   thoughts or feelings about the instrument and to make an overall   evaluation of the measure.</p>
        <p>When analyzing data from the <a href="desgloss.html#Pretest">pretest</a>, the <a href="desgloss.html#Likert-Scales">range of response</a> should be noted; if the responses to an item is limited to a single   response level, the item will be of little use in any type of data   analysis. The correlations between the items and the total score should   be carefully examined; items that do not correlate well with the total   score should be revised or dropped. </p>
        <p>After the scale has been revised based on the subjects' responses and on the <a href="desgloss.html#Pretest">pretest</a> analysis, another <a href="desgloss.html#Pretest">pretest</a> should be  conducted. To avoid any effects from having seen the measure   previously, another sample should be drawn; again the sample should   have characteristics similar to the <a href="desgloss.html#Population-Of-Interest">population of interest</a>. If substantial changes are needed based on the results of the second <a href="desgloss.html#Pretest">pretest</a>, conducting a third <a href="desgloss.html#Pretest">pretest</a> is advisable, and so is conducting as many pretests as needed until the scale requires little or no revision.</p>
        <p>After the final <a href="desgloss.html#Pretest">pretest</a>, data on the reliability and the <a href="desgloss.html#Validity">validity</a> of the measure needs to be collected (see reliability and <a href="desgloss.html#Validity">validity</a>).   The researcher who has constructed a new test can be assured that the   integrity of the scale will be questioned by other investigators or   grant reviewers, so having the appropriate evidence on hand when   criticisms are raised is highly recommended. Further, if the scale is to   be used in a research project and is cited in a grant application,   copies of the scale should be appended to the application, as well as   descriptions about the method of constructing the scale and the   estimates of reliability and <a href="desgloss.html#Validity">validity</a>.</p>
        <p><em>Other considerations</em></p>
        <p>Regardless of whether an investigator is using a scale that has been   newly developed or one that has been used by past  researchers, close   examination of the items and the response categories, if any, is   encouraged before submitting a grant or beginning the investigation.   Often the researcher may find that the items are not appropriate to the <a href="desgloss.html#Population-Of-Interest">population of interest</a>. Sometimes, particularly in new or rarely used scales, the items will bear little relevance to the <a href="desgloss.html#Construct">construct</a> of interest; in such <a href="desgloss.html#Cases">cases</a> the measure may have been developed and a subjective and inappropriate   title applied to the scale.  Therefore, do not assume that a scale is a   good measure or any measure at all of psychological depression merely   because the word &quot;depression&quot; appears in the title of the scale.</p>
        <p>Finally, all or almost all <a href="desgloss.html#Instruments">instruments</a> can be criticized for containing some degree of error. <a href="desgloss.html#Self-Report">Self-report</a> measures are often criticized particularly for yielding &quot;soft&quot; data. However, as long as effort has been taken to <a href="desgloss.html#Construct">construct</a> a good quality test, a researcher should be able to identify   individuals who will can be categorized as being relatively &quot;high&quot;   and/or &quot;low&quot; on some characteristic, and in some research, the   individuals at the extremes are of the most interest.</p>
        <p>Of course, increasing the accuracy of the measurement should always be of interest.    For example, in a study of the prevalence of cigarette smoking in a   teenage population, an investigator may administer a typical set of   questions on smoking, but may also collect sputum samples, telling the   subjects the sputum can be analyzed to detect even the most minute   amounts of nicotine. The researcher may also  have built a <a href="desgloss.html#Payment">payment</a> scheme into the design in which subjects are told that they would not   be paid if their responses were not  consistent with sputum analysis.   However, whenever subjects are deceived, ethical questions may arise.</p>
        <h2><!-- End of Right Column // Page Content --><br class="clearfloat" />
      </h2>
    </div></div></div>
<!-- End of Main Content -->
    
<!-- Last Updated -->
  <div class="update">Last Updated: October 3, 2012</div>
<!-- End of Last Updated -->
    
<!-- Footer -->
  <div id="foot-row1">
	<ul class="flist2">
    <li><a href="/index.html">Site Home</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/contact">Contact Us</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/policies/disclaimer">Disclaimer Policy</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/web/policies/accessibility">Accessibility</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/viewing-files">Viewing Files</a></li>
    <li>|</li>
    <li><a href="http://www.cancer.gov/global/web/policies/foia">FOIA</a></li>
    <li>|</li>
	<li><a href="/help.html">Help</a></li>
    <li>|</li>
    <li><a href="/site-map.html">Site Map</a></li>
  </ul>
	<br />

	<ul class="flist2">
    <li><a href="http://www.hhs.gov/">U.S. Department of Health and Human Services</a></li>
    <li>|</li>
  <li><a href="http://www.nih.gov/">National Institutes of Health</a></li>
  <li>|</li>
  <li><a href="http://www.cancer.gov/">National Cancer Institute</a></li>
  <li>|</li>
  <li><a href="http://www.usa.gov/">USA.gov</a></li>
  </ul>
  <div class="nih-tagline">NIH…Turning Discovery Into Health<sup>&#174;</sup></div>
	</div>
  </div>
<!-- End of Footer -->
</div>

<!-- Image map for head-row1 -->
<map name="Map" id="Map">
	<area shape="rect" coords="12,3,294,38" href="http://www.cancer.gov/" alt="National Cancer Institute" />
	<area shape="rect" coords="715,10,892,33" href="http://www.nih.gov" alt="U.S. National Institutes of Health" />
	<area shape="rect" coords="893,10,988,33" href="http://www.cancer.gov/" alt="National Cancer Institute" />
</map>
<!-- End of Image map for head-row1 -->

<!-- GA Code -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-25589575-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<!-- End of GA Code -->
<script language="JavaScript" type="text/javascript" src="/JS/Omniture/WA_DCCPS_PageLoad.js"></script>
</body>
</html>